# Home_Sales

This challenge involves using **SparkSQL** to determine key metrics about home sales data. Spark was utilized to create temporary views, partition the data, cache and uncache a temporary table, and verify that the table had been uncached.

*This challenge should be included the following:*

This challenge includes the following steps:

1. A Spark DataFrame is created from the dataset.

2. A temporary table is created from the original DataFrame.

3. A query is written to return the average price, rounded to two decimal places, of four-bedroom homes sold in each year.

4. A query is written to return the average price, rounded to two decimal places, of homes with three bedrooms and three bathrooms for each year the home was built.

5. A query is written to return the average price, rounded to two decimal places, of homes with three bedrooms, three bathrooms, two floors, and at least 2,000 square feet, for each year the home was built.

6. A query is written to return the average price per `view` rating, where the average home price is greater than or equal to $350,000, rounded to two decimal places. (The output includes the runtime for this query).

7. A cache of the temporary `home_sales` table is created and validated.

8. The query from step 6 is run on the cached temporary table, and the runtime is recorded.

9. The home sales dataset is partitioned by the `date_built` field, and the formatted parquet data is read.

10. A temporary table is created from the parquet data.

11. The query from step 6 is run on the parquet temporary table, and the runtime is recorded.

12. The `home_sales` temporary table is uncached and verified.


##  Files of repository

- *Home_Sales_colab.ipynb*: the file contains all analysis, using Google colab.



# References
Data for this dataset was generated by edX Boot Camps LLC, and is intended for educational purposes only.